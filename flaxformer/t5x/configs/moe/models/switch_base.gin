# Switch ("top-1 tokens choose experts routing") MoE Base model.
#
# This modified version of the Switch Transformer is more closely aligned with
# T5 v1.1 and more recent MoE models. It uses GEGLU activations with a smaller
# MLP activation dimension, fewer sparse layers in the encoder and decoder,
# a smaller default group size of 4096, and a separate (unshared) output logits
# layer.
#
# For the original Switch Transformer, see models/switch_classic_base.gin.
#
# Provides MODEL

from __gin__ import dynamic_registration

from flaxformer.architectures.moe import moe_layers
from flaxformer.architectures.moe import routing
import seqio
from t5x import adafactor

ARCHITECTURE = %gin.REQUIRED
NUM_EXPERTS = %gin.REQUIRED

include 'flaxformer/t5x/configs/moe/models/tokens_choose_base.gin'

# MoE overrides
TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
EVAL_EXPERT_CAPACITY_FACTOR = 2.
NUM_SELECTED_EXPERTS = 1   # Switch routing
AUX_LOSS_FACTOR = 0.01
ROUTER_Z_LOSS_FACTOR = 0.0

# Switch Transformer doesn't use BPR in encoder (although most sparse encoders
# generally see a boost from it).
sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
