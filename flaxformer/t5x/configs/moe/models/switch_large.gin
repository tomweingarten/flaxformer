# Switch MoE Large model.
#
# This modified version of the Switch Transformer is more closely aligned with
# T5 v1.1 and more recent MoE models. It uses GEGLU activations with a smaller
# MLP activation dimension, fewer sparse layers in the encoder and decoder,
# and a separate (unshared) output logits layer.
#
# For the original Switch Transformer, see models/switch_classic_large.gin.
#
# Provides MODEL

from flaxformer.architectures.moe import moe_layers

include 'flaxformer/t5x/configs/moe/models/switch_base.gin'

# Architecture overrides
NUM_ENCODER_LAYERS = 24
NUM_DECODER_LAYERS = 24
NUM_HEADS = 16
HEAD_DIM = 64
EMBED_DIM = 1024
MLP_DIM = 2816

# MoE overrides
# Replace every fourth dense MLP sublayer with MoE sublayer.
NUM_ENCODER_SPARSE_LAYERS = 6
NUM_DECODER_SPARSE_LAYERS = 6

# Experimental.
# For model parallelism for large models, we can reduce all-to-all
# communication by partitioning inputs before transferring data.
# moe_layers.MoeLayer.optimize_model_parallel_communications = True
