# Switch Transformer Large model.
#
# Based on the original Switch Transformer (https://arxiv.org/abs/2101.03961).
# See also models/switch_large.gin for a more recent variant of the model.
#
# Note that unlike the original Mesh Tensorflow Switch Transformer, this T5X
# version does not use any jitter noise in the router.
#
# Provides MODEL

from flaxformer.architectures.moe import moe_layers

include 'flaxformer/t5x/configs/moe/models/switch_classic_base.gin'

# Architecture overrides
NUM_ENCODER_LAYERS = 24
NUM_DECODER_LAYERS = 24
NUM_HEADS = 16
HEAD_DIM = 64
EMBED_DIM = 1024
MLP_DIM = 4096
GROUP_SIZE = 4096  # Smaller than Base

# MoE overrides
# Replace every other MLP sublayer is an MoE sublayer.
NUM_ENCODER_SPARSE_LAYERS = 12
NUM_DECODER_SPARSE_LAYERS = 12

# Switch Transformer Large uses the same activations and output logits
# projection as the Base config.

# Experimental.
# For model parallelism for large models, we can reduce all-to-all
# communication by partitioning inputs before transferring data.
# moe_layers.MoeLayer.optimize_model_parallel_communications = True

