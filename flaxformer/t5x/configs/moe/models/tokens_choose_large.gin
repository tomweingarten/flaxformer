# Top-2 "tokens choose experts routing" MoE Large model.
# Provides MODEL

from flaxformer.architectures.moe import moe_layers

include 'flaxformer/t5x/configs/moe/models/tokens_choose_base.gin'

# Architecture overrides
NUM_ENCODER_LAYERS = 24
NUM_DECODER_LAYERS = 24
NUM_HEADS = 16
HEAD_DIM = 64
EMBED_DIM = 1024
MLP_DIM = 4096

# MoE overrides
# Replace every fourth dense MLP sublayer with MoE sublayer.
NUM_ENCODER_SPARSE_LAYERS = 6
NUM_DECODER_SPARSE_LAYERS = 6

# Experimental.
# For model parallelism for large models, we can reduce all-to-all
# communication by partitioning inputs before transferring data.
# NUM_MODEL_PARTITIONS = %gin.REQUIRED
# moe_layers.MoeLayer.num_model_partitions = %NUM_MODEL_PARTITIONS
